{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKLnNWSXtSAPGz0CefSlRI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohitSen1235/BioSim/blob/main/catbreed_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Installing the necessary libraries for this project*##"
      ],
      "metadata": {
        "id": "Xg9jPXhzvuuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio\n",
        "!pip install scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwYh4jPIjebT",
        "outputId": "def58ca1-5963-40ff-842e-45a6dde9e5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.18.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.7.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2022.10.10)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.21.6)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.9.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*connecting the notebook with your google drive*##\n",
        "###*This is done so that you can access the data set that is stored in google drive*###"
      ],
      "metadata": {
        "id": "iSBGgkFDv1Wi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsgbJirvaocN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f19e614-d492-427d-cb2a-6d53227c31ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Declaring the path of the files in the drive*##\n",
        "##*Also the path where the model will be saved*##"
      ],
      "metadata": {
        "id": "DSME9refwO3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/Fiverr/vascoreis753/CATS/\"\n",
        "model_dir='/content/drive/MyDrive/Fiverr/vascoreis753/'"
      ],
      "metadata": {
        "id": "ezDxm97gbArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Importing all the necessary libraries and modules for the program*##"
      ],
      "metadata": {
        "id": "uRvfCKPWwdZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.models import Model, Sequential,load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "Ko_5jdUcbejR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Creating an empty list to store all the name of breeds*##"
      ],
      "metadata": {
        "id": "nEC4HL2Vw4p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list to store the breeds\n",
        "breeds = []"
      ],
      "metadata": {
        "id": "SoraEzFGbfzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The naming convention of the dataset is as follows*##\n",
        "####BreedName_number.jpg###\n",
        "###the below code splits the name into two portions### \n",
        "####*1. before the   \"_\"*####\n",
        "####*2. After the    \"_\"*#### \n",
        "###all unique strings before \"_\" are appended to the list of breeds###"
      ],
      "metadata": {
        "id": "iNMGSWPlxFXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through the images in the folder\n",
        "for image in os.listdir(path):\n",
        "    if image.endswith('.jpg') or image.endswith('.jpeg') or image.endswith('.png'):\n",
        "        # extract the breed name from the image name\n",
        "        breed = image.split(\"_\")[0]\n",
        "        # add the breed to the list if it is not already in the list\n",
        "        if breed not in breeds:\n",
        "            breeds.append(breed)\n",
        "\n",
        "\n",
        "print(breeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TG9tdDjblYV",
        "outputId": "200579bb-af0b-4045-c112-40f9a5ba2cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Maine', 'Persian', 'Bombay', 'Sphynx', 'Egyptian', 'Birman', 'British', 'Abyssinian', 'Ragdoll', 'Russian', 'Siamese', 'Bengal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###you can see that there are 12 breeds in the given daaset###"
      ],
      "metadata": {
        "id": "x7BQCcEByUSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*creating a dictionary to map breed to an integer for computation purposes*###"
      ],
      "metadata": {
        "id": "QHgohwbgyeVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary to map breed names to integers\n",
        "breed_mapping = {breed: i for i, breed in enumerate(breeds)}\n",
        "print(breed_mapping)"
      ],
      "metadata": {
        "id": "_1I0UGCxbxzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5560ad-64ac-40dd-fb76-ee11b7403524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Maine': 0, 'Persian': 1, 'Bombay': 2, 'Sphynx': 3, 'Egyptian': 4, 'Birman': 5, 'British': 6, 'Abyssinian': 7, 'Ragdoll': 8, 'Russian': 9, 'Siamese': 10, 'Bengal': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Two empty lists are created for storing the image data and the respective labels*###"
      ],
      "metadata": {
        "id": "CDQuyk-_ysiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the data and labels arrays\n",
        "data = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "gPtvKd3IcOAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Images in the dataset may be of varying size and resolution, to create a model for prediction using Neural Networks all the images should be of a standard size*###\n",
        "####*- resolution of 224 x 224 was chosen for this project, because this works well with the VGG16 model we plan to use for this project*#### \n",
        "####*- all the images should be resized to this resolution before storing into data list*####"
      ],
      "metadata": {
        "id": "ZukCe0IrzMEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through the images in the folder\n",
        "for image in os.listdir(path):\n",
        "    if image.endswith('.jpg') or image.endswith('.jpeg') or image.endswith('.png'):\n",
        "        # load the image\n",
        "        img = cv2.imread(path+image)\n",
        "        # resize the image\n",
        "        try:\n",
        "          # resizing the image\n",
        "          img = cv2.resize(img, (224, 224))\n",
        "          # add the image to the data list\n",
        "          data.append(img)\n",
        "          # extract the breed name from the image name\n",
        "          breed = image.split(\"_\")[0]\n",
        "          # add the corresponding label to the labels list\n",
        "          labels.append(breed_mapping[breed])\n",
        "        except Exception as e:\n",
        "          print(f\"could not resize {image}\")\n",
        "       \n",
        "\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkx-whKPi6W6",
        "outputId": "f22d87ba-a971-47f2-fce5-287b835afd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "could not resize Egyptian_Mau_191.jpg\n",
            "could not resize Egyptian_Mau_177.jpg\n",
            "could not resize Egyptian_Mau_139.jpg\n",
            "could not resize Egyptian_Mau_145.jpg\n",
            "could not resize Abyssinian_34.jpg\n",
            "could not resize Egyptian_Mau_167.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*please note that not all images could be resized, some of the images which are shown above were not resizable hence we had to ignore those*### "
      ],
      "metadata": {
        "id": "Iul1_t5h0NTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total nnumber of images available : {len(data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzJa-w_m25TR",
        "outputId": "fec1042f-4335-4508-add7-db051c12d832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total nnumber of images available : 2394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*In order to train the model we require the data and labels as numpy arrays*###\n",
        "\n",
        "####*- hence converting the python lists : data and labels in to numpy arrays*#### "
      ],
      "metadata": {
        "id": "OfYjIA9f0u3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "print(f\"Data : {data.shape} , lables : {labels.shape}\")"
      ],
      "metadata": {
        "id": "kBzxnXhKnQ7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6d3dd7-9e70-4bd0-9815-32a1395bdeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data : (2394, 224, 224, 3) , lables : (2394,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*In order to make sure that the training set as well as the test set have representatives of all Breeds the data was split into numerous sample sizes randomly*###\n",
        "\n",
        "####*Train and test set are later gathered from this randomized pool of data*####"
      ],
      "metadata": {
        "id": "paGsUcAy1XKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Your data and labels stored in X and y respectively\n",
        "X = data\n",
        "y = labels\n",
        "\n",
        "# Define the number of splits you want\n",
        "n_splits = 15\n",
        "\n",
        "# Create an instance of the StratifiedShuffleSplit class\n",
        "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.15, random_state=42)\n",
        "\n",
        "# Use the split method to get the train and test indices\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "print(f\"training set: {len(X_train)} , test set : {len(X_test)}\")"
      ],
      "metadata": {
        "id": "tpbhXorM7AgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd97540-2ddb-4a21-8d11-2781a7b8dd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set: 2034 , test set : 360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*download the VGG16 model and define the input data shape*###"
      ],
      "metadata": {
        "id": "JKTufFz518XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model of VGG16 model\n",
        "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ],
      "metadata": {
        "id": "WHV35vbdc9cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*- Converting data to tensors is necessary as tensors are the basic data structure used in TensorFlow and are required for building and training a machine learning model.*###\n",
        "\n",
        "###*- Normalizing the data by dividing it by 255.0 helps in scaling the values of the data between 0 and 1, which can help the model learn better.*###\n",
        "\n",
        "###*- One-hot encoding the labels transforms the labels into a binary matrix representation where each column corresponds to a single category. This encoding is needed for multiclass classification problems as it allows the model to predict the class of a sample by providing the probability of each class.*###"
      ],
      "metadata": {
        "id": "1mNX_IkR25xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, len(breeds))\n",
        "y_test = to_categorical(y_test, len(breeds))\n"
      ],
      "metadata": {
        "id": "z_qsu17O5VOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*- This code block freezes the layers of the base VGG16 model, meaning that the weights of these layers will not be updated during training. This helps to prevent overfitting by retaining the pre-trained feature extraction capabilities of the base model.*###"
      ],
      "metadata": {
        "id": "jWKPXWNU3bgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers of the base model\n",
        "for layer in vgg16_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "C7KxCUrldAqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*- The code below creates a Sequential model in Keras.*###\n",
        "###*- The VGG16 model is then added on top of the base model by calling model.add(vgg16_model).*###\n",
        "###*- The Flatten layer is then added, which is used to flatten the multi-dimensional input from the previous layer into a one-dimensional array to be processed by the dense layers.*###\n",
        "###*- The first Dense layer is added with 256 units and ReLU activation. The number of units determines the dimensionality of the output space and ReLU activation is used for the activation function.*###\n",
        "###*- The Dropout layer is added with a rate of 0.5, which is used to prevent overfitting by randomly dropping some neurons during training.*###\n",
        "###*- The final Dense layer is added with the number of units equal to the number of different breeds and with a Softmax activation function. The Softmax activation function is used to ensure that the sum of all outputs is 1, so that the outputs can be interpreted as probabilities.*###\n",
        "###* -The model is then compiled by specifying the optimizer (Adam), loss function (categorical crossentropy), and evaluation metrics (accuracy).*###\n",
        "###* -A ModelCheckpoint is created to save the best model after each epoch during training.*###\n",
        "###* -Finally, the model is fit on the training data for 25 epochs with a batch size of 128, with the validation data and the ModelCheckpoint being passed to the fit method as arguments.*###"
      ],
      "metadata": {
        "id": "TKuhhzet4Ujj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the VGG16 model on top of the base model\n",
        "model.add(vgg16_model)\n",
        "\n",
        "model.add(Flatten())\n",
        "# Add a Dense layer with 128 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# Add a Dropout layer with a rate of 0.5\n",
        "model.add(Dropout(0.4))\n",
        "# Add a Dense layer with the number of units equal to the number of breeds and Softmax activation\n",
        "model.add(Dense(len(breeds), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create a checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint(model_dir+'best_model.h5', save_best_only=True)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test), callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYQBCkDKdGBA",
        "outputId": "f6ee3eb6-3c1b-4ccc-bf12-d5bc7687c7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 14s 856ms/step - loss: 3.3958 - accuracy: 0.1583 - val_loss: 1.9960 - val_accuracy: 0.2889\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 15s 984ms/step - loss: 1.8523 - accuracy: 0.3338 - val_loss: 1.6593 - val_accuracy: 0.4306\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 15s 977ms/step - loss: 1.5180 - accuracy: 0.4685 - val_loss: 1.4634 - val_accuracy: 0.5250\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 14s 895ms/step - loss: 1.2576 - accuracy: 0.5792 - val_loss: 1.3358 - val_accuracy: 0.5639\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 14s 893ms/step - loss: 1.0886 - accuracy: 0.6450 - val_loss: 1.2561 - val_accuracy: 0.5972\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 14s 893ms/step - loss: 0.9212 - accuracy: 0.7104 - val_loss: 1.1602 - val_accuracy: 0.6028\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 14s 866ms/step - loss: 0.8036 - accuracy: 0.7448 - val_loss: 1.1478 - val_accuracy: 0.6278\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 15s 982ms/step - loss: 0.7126 - accuracy: 0.7753 - val_loss: 1.0663 - val_accuracy: 0.6333\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 15s 982ms/step - loss: 0.5912 - accuracy: 0.8299 - val_loss: 1.0296 - val_accuracy: 0.6444\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 14s 878ms/step - loss: 0.4982 - accuracy: 0.8540 - val_loss: 1.0227 - val_accuracy: 0.6583\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 14s 884ms/step - loss: 0.4563 - accuracy: 0.8761 - val_loss: 1.0135 - val_accuracy: 0.6639\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 13s 797ms/step - loss: 0.4088 - accuracy: 0.8850 - val_loss: 1.0247 - val_accuracy: 0.6222\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 14s 889ms/step - loss: 0.3642 - accuracy: 0.9046 - val_loss: 0.9927 - val_accuracy: 0.6611\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 14s 891ms/step - loss: 0.3217 - accuracy: 0.9204 - val_loss: 0.9636 - val_accuracy: 0.6667\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 13s 831ms/step - loss: 0.2868 - accuracy: 0.9267 - val_loss: 0.9558 - val_accuracy: 0.6806\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 13s 802ms/step - loss: 0.2664 - accuracy: 0.9258 - val_loss: 1.0103 - val_accuracy: 0.6278\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 14s 898ms/step - loss: 0.2550 - accuracy: 0.9322 - val_loss: 0.9412 - val_accuracy: 0.6806\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 13s 804ms/step - loss: 0.2279 - accuracy: 0.9444 - val_loss: 0.9723 - val_accuracy: 0.6667\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 13s 794ms/step - loss: 0.2197 - accuracy: 0.9425 - val_loss: 0.9926 - val_accuracy: 0.6639\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 13s 794ms/step - loss: 0.2024 - accuracy: 0.9444 - val_loss: 0.9588 - val_accuracy: 0.6722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6b545c1ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Loading the model to use in predicting some images*###"
      ],
      "metadata": {
        "id": "Y93nZuii42wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the best model\n",
        "model = load_model('/content/drive/MyDrive/Fiverr/vascoreis753/best_model.h5')\n"
      ],
      "metadata": {
        "id": "8yg4S4bwtxVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*loading the images for whihc I want to get the breed*###"
      ],
      "metadata": {
        "id": "EkfobOFC5IgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the image you want to classify\n",
        "# Abyssinian\n",
        "img1 = cv2.imread('/content/drive/MyDrive/Fiverr/vascoreis753/Abyssinian_13.jpg')\n",
        "# Bengal\n",
        "img2 = cv2.imread('/content/drive/MyDrive/Fiverr/vascoreis753/Bengal_109.jpg')\n",
        "#Bombay\n",
        "img3 = cv2.imread('/content/drive/MyDrive/Fiverr/vascoreis753/Bombay_14.jpg')\n",
        "#Spynx\n",
        "img4 = cv2.imread('/content/drive/MyDrive/Fiverr/vascoreis753/spynx.jpg')"
      ],
      "metadata": {
        "id": "2ilsTio-5Haf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Defined a function for finding the breed*###"
      ],
      "metadata": {
        "id": "WdmZBJea5eYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_breed(model, image):\n",
        "  \n",
        "  #Convert the image to a numpy array\n",
        "  img = cv2.resize(image, (224, 224))\n",
        "  \n",
        "  #Expand the dimensions of the image to (1, 224, 224, 3)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  \n",
        "  result_prob = model.predict(img)\n",
        "  result = result_prob.argmax(axis=-1)\n",
        "\n",
        "  predicted_class_index = np.argmax(result_prob)\n",
        "\n",
        "  #Get the label of the predicted breed\n",
        "  predicted_breed = breeds[predicted_class_index]\n",
        "\n",
        "\n",
        "\n",
        "  print(f\"The predicted breed is : {predicted_breed}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xsHRtvYVS48s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should predict Bengal\n",
        "predict_breed(model,img2)\n",
        "# should predict Bombay\n",
        "predict_breed(model,img3)\n",
        "# Should predict Abyssinian\n",
        "predict_breed(model,img1)\n",
        "# Should predict Sphynx\n",
        "predict_breed(model,img4)"
      ],
      "metadata": {
        "id": "Te6xJf8QcFH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e40281-a52c-4301-915a-6810f8af9367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 487ms/step\n",
            "The predicted breed is : Bengal\n",
            "\n",
            "1/1 [==============================] - 0s 383ms/step\n",
            "The predicted breed is : Bombay\n",
            "\n",
            "1/1 [==============================] - 0s 388ms/step\n",
            "The predicted breed is : Abyssinian\n",
            "\n",
            "1/1 [==============================] - 0s 387ms/step\n",
            "The predicted breed is : Sphynx\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*In conclusion, atleast for the cases tried so far the model seems to be predicting the correct breed*###\n",
        "###*However please note that the model accuracy is approximately 69% hance it is bound to make some mistakes in general*###"
      ],
      "metadata": {
        "id": "k38-XCid5_02"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txKIz0Qr6Wtm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}